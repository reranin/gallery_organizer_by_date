#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
Ø§ÛŒØ¬Ø§Ø¯ØŒ Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
"""

import os
import shutil
import hashlib
import argparse
import logging
import json
import zipfile
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø®ØªÛŒØ§Ø±ÛŒ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ tqdm Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ù†ÙˆØ§Ø± Ù¾ÛŒØ´Ø±ÙØª Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

@dataclass
class BackupInfo:
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ"""
    backup_id: str
    source_path: str
    backup_path: str
    timestamp: str
    total_files: int
    total_size: int
    checksum: str
    compression: bool
    status: str = "created"

class BackupManager:
    """Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ"""
    
    def __init__(self, backup_root: str = None):
        if backup_root is None:
            backup_root = os.getenv("BACKUP_ROOT_DIR", "./backups")
        self.backup_root = Path(backup_root)
        self.backup_root.mkdir(parents=True, exist_ok=True)
        self.setup_logging()
        
        # ÙØ§ÛŒÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§
        self.index_file = self.backup_root / "backup_index.json"
        self.backups_index = self.load_backup_index()
        
        # Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ
        self.supported_extensions = {
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.heic',
            '.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg',
            '.pdf', '.doc', '.docx', '.txt', '.zip', '.rar'
        }
    
    def setup_logging(self):
        """ØªÙ†Ø¸ÛŒÙ… Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.backup_root / f'backup_manager_{timestamp}.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_backup_index(self) -> List[BackupInfo]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§"""
        if not self.index_file.exists():
            return []
        
        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return [BackupInfo(**item) for item in data]
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³: {e}")
            return []
    
    def save_backup_index(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§"""
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                data = [asdict(backup) for backup in self.backups_index]
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³: {e}")
    
    def calculate_checksum(self, file_path: Path) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ checksum ÙØ§ÛŒÙ„"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def get_directory_info(self, directory: Path) -> Tuple[int, int]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÙˆØ´Ù‡ (ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ù„)"""
        total_files = 0
        total_size = 0
        
        for file_path in directory.rglob("*"):
            if file_path.is_file():
                if file_path.suffix.lower() in self.supported_extensions:
                    total_files += 1
                    try:
                        total_size += file_path.stat().st_size
                    except:
                        pass
        
        return total_files, total_size
    
    def create_backup_simple(self, source_path: str, backup_name: str = None) -> BackupInfo:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø³Ø§Ø¯Ù‡ (Ú©Ù¾ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…)"""
        source = Path(source_path)
        if not source.exists():
            raise FileNotFoundError(f"Ù…Ø³ÛŒØ± Ù…Ù†Ø¨Ø¹ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {source_path}")
        
        # ØªÙˆÙ„ÛŒØ¯ Ù†Ø§Ù… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if backup_name:
            backup_id = f"{backup_name}_{timestamp}"
        else:
            backup_id = f"backup_{source.name}_{timestamp}"
        
        backup_dir = self.backup_root / backup_id
        backup_dir.mkdir(exist_ok=True)
        
        self.logger.info(f"Ø´Ø±ÙˆØ¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ: {source} -> {backup_dir}")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        if source.is_file():
            total_files = 1
            total_size = source.stat().st_size
        else:
            total_files, total_size = self.get_directory_info(source)
        
        # Ú©Ù¾ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        copied_files = 0
        if source.is_file():
            shutil.copy2(source, backup_dir / source.name)
            copied_files = 1
        else:
            if TQDM_AVAILABLE:
                progress_bar = tqdm(total=total_files, desc="Ú©Ù¾ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§")
            else:
                progress_bar = None
            
            for file_path in source.rglob("*"):
                if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                    try:
                        # Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø± Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
                        relative_path = file_path.relative_to(source)
                        dest_path = backup_dir / relative_path
                        dest_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        shutil.copy2(file_path, dest_path)
                        copied_files += 1
                        
                        if progress_bar:
                            progress_bar.update(1)
                            
                    except Exception as e:
                        self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ú©Ù¾ÛŒ {file_path}: {e}")
            
            if progress_bar:
                progress_bar.close()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ checksum Ù¾ÙˆØ´Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
        backup_checksum = self.calculate_directory_checksum(backup_dir)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
        backup_info = BackupInfo(
            backup_id=backup_id,
            source_path=str(source),
            backup_path=str(backup_dir),
            timestamp=timestamp,
            total_files=copied_files,
            total_size=total_size,
            checksum=backup_checksum,
            compression=False,
            status="completed"
        )
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        self.backups_index.append(backup_info)
        self.save_backup_index()
        
        self.logger.info(f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯: {copied_files} ÙØ§ÛŒÙ„")
        return backup_info
    
    def create_backup_compressed(self, source_path: str, backup_name: str = None) -> BackupInfo:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÙØ´Ø±Ø¯Ù‡ (ZIP)"""
        source = Path(source_path)
        if not source.exists():
            raise FileNotFoundError(f"Ù…Ø³ÛŒØ± Ù…Ù†Ø¨Ø¹ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {source_path}")
        
        # ØªÙˆÙ„ÛŒØ¯ Ù†Ø§Ù… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if backup_name:
            backup_id = f"{backup_name}_{timestamp}"
        else:
            backup_id = f"backup_{source.name}_{timestamp}"
        
        backup_file = self.backup_root / f"{backup_id}.zip"
        
        self.logger.info(f"Ø´Ø±ÙˆØ¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÙØ´Ø±Ø¯Ù‡: {source} -> {backup_file}")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        if source.is_file():
            total_files = 1
            total_size = source.stat().st_size
        else:
            total_files, total_size = self.get_directory_info(source)
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ ZIP
        compressed_files = 0
        with zipfile.ZipFile(backup_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
            if source.is_file():
                zipf.write(source, source.name)
                compressed_files = 1
            else:
                if TQDM_AVAILABLE:
                    progress_bar = tqdm(total=total_files, desc="ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§")
                else:
                    progress_bar = None
                
                for file_path in source.rglob("*"):
                    if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                        try:
                            # Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø± Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ZIP
                            arcname = file_path.relative_to(source)
                            zipf.write(file_path, arcname)
                            compressed_files += 1
                            
                            if progress_bar:
                                progress_bar.update(1)
                                
                        except Exception as e:
                            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ {file_path}: {e}")
                
                if progress_bar:
                    progress_bar.close()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ checksum ÙØ§ÛŒÙ„ ZIP
        backup_checksum = self.calculate_checksum(backup_file)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
        backup_info = BackupInfo(
            backup_id=backup_id,
            source_path=str(source),
            backup_path=str(backup_file),
            timestamp=timestamp,
            total_files=compressed_files,
            total_size=backup_file.stat().st_size,
            checksum=backup_checksum,
            compression=True,
            status="completed"
        )
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        self.backups_index.append(backup_info)
        self.save_backup_index()
        
        self.logger.info(f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÙØ´Ø±Ø¯Ù‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯: {compressed_files} ÙØ§ÛŒÙ„")
        return backup_info
    
    def calculate_directory_checksum(self, directory: Path) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ checksum Ù¾ÙˆØ´Ù‡"""
        hash_md5 = hashlib.md5()
        
        try:
            for file_path in sorted(directory.rglob("*")):
                if file_path.is_file():
                    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… ÙØ§ÛŒÙ„
                    hash_md5.update(str(file_path.relative_to(directory)).encode('utf-8'))
                    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„
                    with open(file_path, 'rb') as f:
                        for chunk in iter(lambda: f.read(4096), b""):
                            hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def verify_backup(self, backup_id: str) -> Dict:
        """ØªØ§ÛŒÛŒØ¯ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ"""
        backup_info = self.get_backup_info(backup_id)
        if not backup_info:
            return {"error": f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯: {backup_id}"}
        
        backup_path = Path(backup_info.backup_path)
        if not backup_path.exists():
            return {"error": f"ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {backup_path}"}
        
        self.logger.info(f"ØªØ§ÛŒÛŒØ¯ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ: {backup_id}")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ checksum Ø¬Ø¯ÛŒØ¯
        if backup_info.compression:
            current_checksum = self.calculate_checksum(backup_path)
        else:
            current_checksum = self.calculate_directory_checksum(backup_path)
        
        # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ checksum Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        is_intact = current_checksum == backup_info.checksum
        
        result = {
            "backup_id": backup_id,
            "is_intact": is_intact,
            "original_checksum": backup_info.checksum,
            "current_checksum": current_checksum,
            "backup_path": str(backup_path),
            "compression": backup_info.compression
        }
        
        if is_intact:
            self.logger.info("Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø³Ø§Ù„Ù… Ø§Ø³Øª âœ…")
        else:
            self.logger.warning("Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®Ø±Ø§Ø¨ Ø§Ø³Øª âš ï¸")
        
        return result
    
    def restore_backup(self, backup_id: str, restore_path: str) -> Dict:
        """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ"""
        backup_info = self.get_backup_info(backup_id)
        if not backup_info:
            return {"error": f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯: {backup_id}"}
        
        backup_path = Path(backup_info.backup_path)
        if not backup_path.exists():
            return {"error": f"ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {backup_path}"}
        
        restore_dir = Path(restore_path)
        restore_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ {backup_id} Ø¨Ù‡ {restore_path}")
        
        try:
            if backup_info.compression:
                # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ ZIP
                with zipfile.ZipFile(backup_path, 'r') as zipf:
                    zipf.extractall(restore_dir)
                    restored_files = len(zipf.namelist())
            else:
                # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² Ù¾ÙˆØ´Ù‡
                restored_files = 0
                for file_path in backup_path.rglob("*"):
                    if file_path.is_file():
                        relative_path = file_path.relative_to(backup_path)
                        dest_path = restore_dir / relative_path
                        dest_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(file_path, dest_path)
                        restored_files += 1
            
            self.logger.info(f"Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯: {restored_files} ÙØ§ÛŒÙ„")
            
            return {
                "backup_id": backup_id,
                "restore_path": str(restore_dir),
                "restored_files": restored_files,
                "success": True
            }
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ: {e}")
            return {"error": f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ: {str(e)}"}
    
    def list_backups(self) -> List[Dict]:
        """Ù„ÛŒØ³Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§"""
        backups_list = []
        for backup in self.backups_index:
            backup_dict = asdict(backup)
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
            backup_path = Path(backup.backup_path)
            backup_dict['exists'] = backup_path.exists()
            if backup_dict['exists']:
                backup_dict['current_size'] = backup_path.stat().st_size
            else:
                backup_dict['current_size'] = 0
            backups_list.append(backup_dict)
        
        return backups_list
    
    def get_backup_info(self, backup_id: str) -> Optional[BackupInfo]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ"""
        for backup in self.backups_index:
            if backup.backup_id == backup_id:
                return backup
        return None
    
    def delete_backup(self, backup_id: str) -> Dict:
        """Ø­Ø°Ù Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ"""
        backup_info = self.get_backup_info(backup_id)
        if not backup_info:
            return {"error": f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯: {backup_id}"}
        
        backup_path = Path(backup_info.backup_path)
        
        try:
            if backup_path.exists():
                if backup_path.is_file():
                    backup_path.unlink()
                else:
                    shutil.rmtree(backup_path)
            
            # Ø­Ø°Ù Ø§Ø² Ø§ÛŒÙ†Ø¯Ú©Ø³
            self.backups_index = [b for b in self.backups_index if b.backup_id != backup_id]
            self.save_backup_index()
            
            self.logger.info(f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø­Ø°Ù Ø´Ø¯: {backup_id}")
            return {"success": True, "message": f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ {backup_id} Ø­Ø°Ù Ø´Ø¯"}
            
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ: {e}")
            return {"error": f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù: {str(e)}"}
    
    def cleanup_old_backups(self, keep_count: int = 5) -> Dict:
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        if len(self.backups_index) <= keep_count:
            return {"message": "ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§ Ú©Ù…ØªØ± Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² Ø§Ø³Øª"}
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
        sorted_backups = sorted(self.backups_index, key=lambda x: x.timestamp, reverse=True)
        backups_to_delete = sorted_backups[keep_count:]
        
        deleted_count = 0
        for backup in backups_to_delete:
            result = self.delete_backup(backup.backup_id)
            if result.get("success"):
                deleted_count += 1
        
        return {
            "deleted_count": deleted_count,
            "remaining_count": len(self.backups_index),
            "message": f"{deleted_count} Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø­Ø°Ù Ø´Ø¯"
        }
    
    def generate_report(self) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.backup_root / f"backup_report_{timestamp}.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("Ú¯Ø²Ø§Ø±Ø´ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ØªØ§Ø±ÛŒØ® Ú¯Ø²Ø§Ø±Ø´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§: {len(self.backups_index)}\n\n")
            
            for backup in self.backups_index:
                f.write(f"Ø´Ù†Ø§Ø³Ù‡: {backup.backup_id}\n")
                f.write(f"Ù…Ù†Ø¨Ø¹: {backup.source_path}\n")
                f.write(f"Ù…Ø³ÛŒØ±: {backup.backup_path}\n")
                f.write(f"ØªØ§Ø±ÛŒØ®: {backup.timestamp}\n")
                f.write(f"ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {backup.total_files}\n")
                f.write(f"Ø§Ù†Ø¯Ø§Ø²Ù‡: {backup.total_size:,} Ø¨Ø§ÛŒØª\n")
                f.write(f"ÙØ´Ø±Ø¯Ù‡: {'Ø¨Ù„Ù‡' if backup.compression else 'Ø®ÛŒØ±'}\n")
                f.write(f"ÙˆØ¶Ø¹ÛŒØª: {backup.status}\n")
                f.write("-" * 30 + "\n")
        
        return str(report_path)


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    parser = argparse.ArgumentParser(description="Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    parser.add_argument("-r", "--root", help="Ù¾ÙˆØ´Ù‡ Ø±ÛŒØ´Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ (Ø§Ø² .env Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)")
    
    subparsers = parser.add_subparsers(dest="action", help="Ø¹Ù…Ù„ÛŒØ§Øªâ€ŒÙ‡Ø§")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
    create_parser = subparsers.add_parser("create", help="Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    create_parser.add_argument("source", nargs='?', help="Ù…Ø³ÛŒØ± Ù…Ù†Ø¨Ø¹ (Ø§Ø² .env Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)")
    create_parser.add_argument("-n", "--name", help="Ù†Ø§Ù… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    create_parser.add_argument("-c", "--compress", action="store_true", help="ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ")
    
    # Ù„ÛŒØ³Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§
    subparsers.add_parser("list", help="Ù„ÛŒØ³Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§")
    
    # ØªØ§ÛŒÛŒØ¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
    verify_parser = subparsers.add_parser("verify", help="ØªØ§ÛŒÛŒØ¯ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ")
    verify_parser.add_argument("backup_id", help="Ø´Ù†Ø§Ø³Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    
    # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
    restore_parser = subparsers.add_parser("restore", help="Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    restore_parser.add_argument("backup_id", help="Ø´Ù†Ø§Ø³Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    restore_parser.add_argument("restore_path", help="Ù…Ø³ÛŒØ± Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ")
    
    # Ø­Ø°Ù Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ
    delete_parser = subparsers.add_parser("delete", help="Ø­Ø°Ù Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    delete_parser.add_argument("backup_id", help="Ø´Ù†Ø§Ø³Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    
    # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ
    cleanup_parser = subparsers.add_parser("cleanup", help="Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ")
    cleanup_parser.add_argument("-k", "--keep", type=int, default=5, help="ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ")
    
    # Ú¯Ø²Ø§Ø±Ø´
    subparsers.add_parser("report", help="ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´")
    
    args = parser.parse_args()
    
    if not args.action:
        parser.print_help()
        return 1
    
    print("ğŸ’¾ Ù…Ø¯ÛŒØ± Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    print("=" * 40)
    
    try:
        manager = BackupManager(args.root)
        
        if args.action == "create":
            # ØªØ¹ÛŒÛŒÙ† Ù…Ø³ÛŒØ± Ù…Ù†Ø¨Ø¹
            source_path = args.source or os.getenv("INPUT_DIRECTORY")
            if not source_path:
                print("âŒ Ø®Ø·Ø§: Ù…Ø³ÛŒØ± Ù…Ù†Ø¨Ø¹ Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡")
                print("Ù„Ø·ÙØ§Ù‹ Ù…Ø³ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¯Ø± ÙØ§ÛŒÙ„ .env ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:")
                print("INPUT_DIRECTORY=/path/to/source")
                return 1
            
            # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
            use_compression = args.compress
            if not args.compress:
                # Ø§Ú¯Ø± Ø§Ø² Ø®Ø· ÙØ±Ù…Ø§Ù† Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ØŒ Ø§Ø² .env Ø¨Ø®ÙˆØ§Ù†
                use_compression = os.getenv("DEFAULT_COMPRESSION", "false").lower() in {"1", "true", "yes"}
            
            print(f"ğŸ“‚ Ù…Ù†Ø¨Ø¹: {source_path}")
            print(f"ğŸ’¾ Ù†ÙˆØ¹: {'ÙØ´Ø±Ø¯Ù‡' if use_compression else 'Ø³Ø§Ø¯Ù‡'}")
            
            if use_compression:
                backup_info = manager.create_backup_compressed(source_path, args.name)
            else:
                backup_info = manager.create_backup_simple(source_path, args.name)
            
            print(f"\nâœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯:")
            print(f"Ø´Ù†Ø§Ø³Ù‡: {backup_info.backup_id}")
            print(f"Ù…Ø³ÛŒØ±: {backup_info.backup_path}")
            print(f"ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {backup_info.total_files}")
            print(f"Ø§Ù†Ø¯Ø§Ø²Ù‡: {backup_info.total_size:,} Ø¨Ø§ÛŒØª")
        
        elif args.action == "list":
            backups = manager.list_backups()
            if not backups:
                print("Ù‡ÛŒÚ† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            else:
                print(f"ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§: {len(backups)}")
                print()
                for backup in backups:
                    status = "âœ…" if backup['exists'] else "âŒ"
                    print(f"{status} {backup['backup_id']}")
                    print(f"   Ù…Ù†Ø¨Ø¹: {backup['source_path']}")
                    print(f"   ØªØ§Ø±ÛŒØ®: {backup['timestamp']}")
                    print(f"   ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {backup['total_files']}")
                    print(f"   Ø§Ù†Ø¯Ø§Ø²Ù‡: {backup['current_size']:,} Ø¨Ø§ÛŒØª")
                    print()
        
        elif args.action == "verify":
            result = manager.verify_backup(args.backup_id)
            if "error" in result:
                print(f"âŒ {result['error']}")
                return 1
            
            status = "âœ… Ø³Ø§Ù„Ù…" if result['is_intact'] else "âŒ Ø®Ø±Ø§Ø¨"
            print(f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ: {result['backup_id']}")
            print(f"ÙˆØ¶Ø¹ÛŒØª: {status}")
            if not result['is_intact']:
                print(f"Checksum Ø§ØµÙ„ÛŒ: {result['original_checksum']}")
                print(f"Checksum ÙØ¹Ù„ÛŒ: {result['current_checksum']}")
        
        elif args.action == "restore":
            result = manager.restore_backup(args.backup_id, args.restore_path)
            if "error" in result:
                print(f"âŒ {result['error']}")
                return 1
            
            print(f"âœ… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…ÙˆÙÙ‚:")
            print(f"Ù…Ø³ÛŒØ±: {result['restore_path']}")
            print(f"ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {result['restored_files']}")
        
        elif args.action == "delete":
            result = manager.delete_backup(args.backup_id)
            if "error" in result:
                print(f"âŒ {result['error']}")
                return 1
            
            print(f"âœ… {result['message']}")
        
        elif args.action == "cleanup":
            keep_count = args.keep or int(os.getenv("KEEP_BACKUP_COUNT", "5"))
            result = manager.cleanup_old_backups(keep_count)
            print(f"ğŸ“Š {result['message']}")
            print(f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {result['remaining_count']}")
        
        elif args.action == "report":
            report_path = manager.generate_report()
            print(f"ğŸ“„ Ú¯Ø²Ø§Ø±Ø´ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {report_path}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
        return 1
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

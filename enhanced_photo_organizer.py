#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø¹Ú©Ø³ Ùˆ ÙÛŒÙ„Ù… Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯ Ø¯Ø³ØªÚ¯Ø§Ù‡
Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² 5 Ø¯ÙˆØ±Ø¨ÛŒÙ† Ùˆ Ù…ÙˆØ¨Ø§ÛŒÙ„ Ù…Ø®ØªÙ„Ù Ø¨Ø§ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ®
"""

import os
import sys
import shutil
import logging
import hashlib
import yaml
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
from collections import defaultdict
import time

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ§Ø¯ÛŒØªØ§
try:
    from PIL import Image
    from PIL.ExifTags import TAGS, GPSTAGS
except ImportError:
    print("Ø®Ø·Ø§: Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Pillow Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
    print("Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨: pip install Pillow")
    sys.exit(1)

try:
    from hachoir.parser import createParser
    from hachoir.metadata import extractMetadata
except ImportError:
    print("Ø®Ø·Ø§: Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ hachoir Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
    print("Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨: pip install hachoir")
    sys.exit(1)

try:
    import exifread
except ImportError:
    print("Ø®Ø·Ø§: Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ exifread Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª!")
    print("Ø¨Ø±Ø§ÛŒ Ù†ØµØ¨: pip install exifread")
    sys.exit(1)


class EnhancedPhotoOrganizer:
    """Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø¹Ú©Ø³ Ùˆ ÙÛŒÙ„Ù…"""
    
    def __init__(self, config_file: str = "device_config.yaml"):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ"""
        self.load_config(config_file)
        self.setup_logging()
        self.stats = {
            'total_files': 0,
            'processed': 0,
            'organized': 0,
            'duplicates': 0,
            'errors': 0,
            'by_device': defaultdict(int),
            'by_date': defaultdict(int),
            'by_type': defaultdict(int)
        }
        
    def load_config(self, config_file: str):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„ YAML"""
        config_path = Path(config_file)
        if not config_path.exists():
            self.logger.error(f"ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ {config_file} ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            self.create_default_config()
        
        with open(config_file, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        self.devices = self.config.get('devices', {})
        self.organization = self.config.get('organization', {})
        self.supported_formats = self.config.get('supported_formats', {})
        self.processing = self.config.get('processing', {})
        
    def create_default_config(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶"""
        self.config = {
            'devices': {},
            'organization': {
                'folder_structure': 'YYYY/YYYY-MM-DD/Device',
                'max_files_per_folder': 500,
                'merge_daily_events': True,
                'separate_raw_folder': True
            },
            'supported_formats': {
                'images': ['.jpg', '.jpeg', '.png', '.heic'],
                'videos': ['.mp4', '.mov', '.avi'],
                'raw': ['.cr2', '.nef', '.arw', '.dng']
            },
            'processing': {
                'use_metadata': True,
                'parallel_processing': True,
                'max_workers': 4
            }
        }
        
    def setup_logging(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"photo_organizer_{timestamp}.log"
        
        # ÙØ±Ù…Øª Ù„Ø§Ú¯
        log_format = '%(asctime)s | %(levelname)-8s | %(funcName)-20s | %(message)s'
        date_format = '%Y-%m-%d %H:%M:%S'
        
        # ØªÙ†Ø¸ÛŒÙ… handlers
        handlers = [
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
        
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            datefmt=date_format,
            handlers=handlers
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("=" * 80)
        self.logger.info("Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø¹Ú©Ø³ Ùˆ ÙÛŒÙ„Ù… - Ø´Ø±ÙˆØ¹")
        self.logger.info(f"ÙØ§ÛŒÙ„ Ù„Ø§Ú¯: {log_file}")
        self.logger.info("=" * 80)
        
    def detect_device(self, file_path: Path) -> Optional[str]:
        """ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø§Ø² Ù…ØªØ§Ø¯ÛŒØªØ§"""
        try:
            metadata = self.extract_all_metadata(file_path)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ù†Ø¯Ù‡ Ùˆ Ù…Ø¯Ù„
            manufacturer = metadata.get('make', '').lower()
            model = metadata.get('model', '')
            software = metadata.get('software', '').lower()
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø± Ø¯Ø³ØªÚ¯Ø§Ù‡ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡
            for device_id, device_info in self.devices.items():
                # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ù†Ø¯Ù‡
                if device_info.get('manufacturer', '').lower() in manufacturer:
                    return device_id
                    
                # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
                if 'models' in device_info:
                    for device_model in device_info['models']:
                        if device_model.lower() in model.lower():
                            return device_id
                            
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±
                if 'software_patterns' in device_info:
                    for pattern in device_info['software_patterns']:
                        if pattern.lower() in software:
                            return device_id
                            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø³ÙˆÙ†Ø¯ RAW Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÚ¯Ø§Ù‡
            extension = file_path.suffix.lower()
            for device_id, device_info in self.devices.items():
                if extension in device_info.get('raw_extensions', []):
                    return device_id
                    
            return None
            
        except Exception as e:
            self.logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÚ¯Ø§Ù‡ {file_path.name}: {e}")
            return None
            
    def extract_all_metadata(self, file_path: Path) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ù…ØªØ§Ø¯ÛŒØªØ§ Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹"""
        metadata = {}
        
        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Pillow (Ø¨Ø±Ø§ÛŒ Ø¹Ú©Ø³â€ŒÙ‡Ø§)
        if self.is_image(file_path):
            pil_metadata = self.extract_pil_metadata(file_path)
            if pil_metadata:
                metadata.update(pil_metadata)
                
        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ exifread
        exif_metadata = self.extract_exifread_metadata(file_path)
        if exif_metadata:
            metadata.update(exif_metadata)
            
        # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ hachoir
        hachoir_metadata = self.extract_hachoir_metadata(file_path)
        if hachoir_metadata:
            metadata.update(hachoir_metadata)
            
        return metadata
        
    def extract_pil_metadata(self, file_path: Path) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§ Ø¨Ø§ PIL"""
        try:
            image = Image.open(file_path)
            exifdata = image.getexif()
            
            if not exifdata:
                return None
                
            metadata = {}
            
            for tag_id, value in exifdata.items():
                tag = TAGS.get(tag_id, tag_id)
                
                # ØªØ¨Ø¯ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Øµ
                if tag == "DateTime":
                    try:
                        metadata['datetime'] = datetime.strptime(str(value), "%Y:%m:%d %H:%M:%S")
                    except:
                        metadata['datetime'] = value
                elif tag == "DateTimeOriginal":
                    try:
                        metadata['datetime_original'] = datetime.strptime(str(value), "%Y:%m:%d %H:%M:%S")
                    except:
                        metadata['datetime_original'] = value
                elif tag == "Make":
                    metadata['make'] = str(value)
                elif tag == "Model":
                    metadata['model'] = str(value)
                elif tag == "Software":
                    metadata['software'] = str(value)
                else:
                    metadata[tag] = value
                    
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ GPS
            if hasattr(image, '_getexif'):
                exif = image._getexif()
                if exif and 34853 in exif:  # GPSInfo tag
                    gps_data = {}
                    for key in exif[34853].keys():
                        decode = GPSTAGS.get(key, key)
                        gps_data[decode] = exif[34853][key]
                    metadata['gps'] = gps_data
                    
            return metadata
            
        except Exception as e:
            self.logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± PIL metadata: {e}")
            return None
            
    def extract_exifread_metadata(self, file_path: Path) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§ Ø¨Ø§ exifread"""
        try:
            with open(file_path, 'rb') as f:
                tags = exifread.process_file(f, details=False)
                
            if not tags:
                return None
                
            metadata = {}
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù…
            important_tags = {
                'EXIF DateTimeOriginal': 'datetime_original',
                'EXIF DateTimeDigitized': 'datetime_digitized',
                'Image DateTime': 'datetime',
                'Image Make': 'make',
                'Image Model': 'model',
                'Image Software': 'software',
                'EXIF LensModel': 'lens_model',
                'EXIF FocalLength': 'focal_length',
                'EXIF ISOSpeedRatings': 'iso',
                'EXIF ExposureTime': 'exposure_time',
                'EXIF FNumber': 'f_number'
            }
            
            for exif_tag, meta_key in important_tags.items():
                if exif_tag in tags:
                    value = str(tags[exif_tag])
                    
                    # ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ®
                    if 'DateTime' in exif_tag:
                        try:
                            metadata[meta_key] = datetime.strptime(value, "%Y:%m:%d %H:%M:%S")
                        except:
                            metadata[meta_key] = value
                    else:
                        metadata[meta_key] = value
                        
            return metadata
            
        except Exception as e:
            self.logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± exifread metadata: {e}")
            return None
            
    def extract_hachoir_metadata(self, file_path: Path) -> Optional[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§Ø¯ÛŒØªØ§ Ø¨Ø§ hachoir"""
        try:
            parser = createParser(str(file_path))
            if not parser:
                return None
                
            with parser:
                hachoir_meta = extractMetadata(parser)
                if not hachoir_meta:
                    return None
                    
            metadata = {}
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            if hasattr(hachoir_meta, 'creation_date'):
                metadata['creation_date'] = hachoir_meta.creation_date
            if hasattr(hachoir_meta, 'last_modification'):
                metadata['last_modification'] = hachoir_meta.last_modification
            if hasattr(hachoir_meta, 'camera_manufacturer'):
                metadata['make'] = hachoir_meta.camera_manufacturer
            if hasattr(hachoir_meta, 'camera_model'):
                metadata['model'] = hachoir_meta.camera_model
            if hasattr(hachoir_meta, 'width'):
                metadata['width'] = hachoir_meta.width
            if hasattr(hachoir_meta, 'height'):
                metadata['height'] = hachoir_meta.height
            if hasattr(hachoir_meta, 'duration'):
                metadata['duration'] = hachoir_meta.duration
                
            return metadata
            
        except Exception as e:
            self.logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± hachoir metadata: {e}")
            return None
            
    def get_file_date(self, file_path: Path) -> Optional[datetime]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ§Ø±ÛŒØ® Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø² ÙØ§ÛŒÙ„"""
        metadata = self.extract_all_metadata(file_path)
        
        # Ø§ÙˆÙ„ÙˆÛŒØª ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§
        date_fields = [
            'datetime_original',
            'datetime_digitized', 
            'datetime',
            'creation_date',
            'last_modification'
        ]
        
        for field in date_fields:
            if field in metadata and metadata[field]:
                date_value = metadata[field]
                if isinstance(date_value, datetime):
                    return date_value
                    
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„
        filename_date = self.extract_date_from_filename(file_path)
        if filename_date:
            return filename_date
            
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø±ÛŒØ® Ø³ÛŒØ³ØªÙ… ÙØ§ÛŒÙ„
        return datetime.fromtimestamp(file_path.stat().st_mtime)
        
    def extract_date_from_filename(self, file_path: Path) -> Optional[datetime]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„"""
        filename = file_path.stem
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ ØªØ§Ø±ÛŒØ® Ø¯Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„
        patterns = [
            (r'(\d{4})[-_](\d{2})[-_](\d{2})[-_](\d{2})[-_](\d{2})[-_](\d{2})', '%Y-%m-%d-%H-%M-%S'),
            (r'(\d{8})[-_](\d{6})', '%Y%m%d-%H%M%S'),
            (r'IMG[-_](\d{8})[-_](\d{6})', '%Y%m%d-%H%M%S'),
            (r'VID[-_](\d{8})[-_](\d{6})', '%Y%m%d-%H%M%S'),
            (r'(\d{4})(\d{2})(\d{2})[-_](\d{2})(\d{2})(\d{2})', '%Y%m%d-%H%M%S'),
        ]
        
        for pattern, date_format in patterns:
            match = re.search(pattern, filename)
            if match:
                try:
                    date_str = '-'.join(match.groups())
                    return datetime.strptime(date_str, date_format)
                except:
                    continue
                    
        return None
        
    def is_image(self, file_path: Path) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ ØªØµÙˆÛŒØ±ÛŒ"""
        return file_path.suffix.lower() in self.supported_formats.get('images', [])
        
    def is_video(self, file_path: Path) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ"""
        return file_path.suffix.lower() in self.supported_formats.get('videos', [])
        
    def is_raw(self, file_path: Path) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ RAW"""
        return file_path.suffix.lower() in self.supported_formats.get('raw', [])
        
    def calculate_file_hash(self, file_path: Path, chunk_size: int = 8192) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø´ ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØªÚ©Ø±Ø§Ø±ÛŒ"""
        hash_md5 = hashlib.md5()
        
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(chunk_size), b""):
                hash_md5.update(chunk)
                
        return hash_md5.hexdigest()
        
    def is_duplicate(self, file_path: Path, existing_files: Dict[str, Path]) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† ÙØ§ÛŒÙ„"""
        file_hash = self.calculate_file_hash(file_path)
        return file_hash in existing_files
        
    def create_folder_structure(self, base_path: Path, file_date: datetime, 
                               device: Optional[str] = None) -> Path:
        """Ø§ÛŒØ¬Ø§Ø¯ Ø³Ø§Ø®ØªØ§Ø± ÙÙˆÙ„Ø¯Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® Ùˆ Ø¯Ø³ØªÚ¯Ø§Ù‡"""
        # Ø³Ø§Ø®ØªØ§Ø±: Ø³Ø§Ù„/Ù…Ø§Ù‡/Ø±ÙˆØ²/Ø¯Ø³ØªÚ¯Ø§Ù‡
        year = file_date.strftime("%Y")
        month = file_date.strftime("%m-%B")
        day = file_date.strftime("%d")
        date_folder = file_date.strftime("%Y-%m-%d")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø³ÛŒØ± Ø§ØµÙ„ÛŒ
        if self.organization.get('merge_daily_events', True):
            # Ù‡Ù…Ù‡ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ ÛŒÚ© Ø±ÙˆØ² Ø¯Ø± ÛŒÚ© ÙÙˆÙ„Ø¯Ø±
            folder_path = base_path / year / month / date_folder
        else:
            folder_path = base_path / year / month / day
            
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙÙˆÙ„Ø¯Ø± Ø¯Ø³ØªÚ¯Ø§Ù‡
        if device and device in self.devices:
            device_info = self.devices[device]
            device_folder = device_info.get('folder_prefix', device)
            folder_path = folder_path / device_folder
            
        # ÙÙˆÙ„Ø¯Ø± Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ RAW
        if self.is_raw(file_path) and self.organization.get('separate_raw_folder', True):
            folder_path = folder_path / "RAW"
            
        # Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø±
        folder_path.mkdir(parents=True, exist_ok=True)
        
        return folder_path
        
    def organize_single_file(self, file_path: Path, destination: Path, 
                            existing_hashes: Dict[str, Path]) -> Dict:
        """Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ ÛŒÚ© ÙØ§ÛŒÙ„"""
        result = {
            'file': file_path.name,
            'status': 'pending',
            'device': None,
            'date': None,
            'destination': None,
            'error': None
        }
        
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙØ±Ù…Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡
            if not (self.is_image(file_path) or self.is_video(file_path) or self.is_raw(file_path)):
                result['status'] = 'unsupported'
                result['error'] = 'ÙØ±Ù…Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯'
                return result
                
            # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
            file_hash = self.calculate_file_hash(file_path)
            if file_hash in existing_hashes:
                result['status'] = 'duplicate'
                result['error'] = f'ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø² {existing_hashes[file_hash]}'
                
                # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ ÙÙˆÙ„Ø¯Ø± ØªÚ©Ø±Ø§Ø±ÛŒ
                dup_folder = destination / "Duplicates"
                dup_folder.mkdir(exist_ok=True)
                
                # Ø§ÛŒØ¬Ø§Ø¯ batch folder Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
                batch_num = (len(list(dup_folder.glob("*"))) // 500) + 1
                batch_folder = dup_folder / f"batch_{batch_num:03d}"
                batch_folder.mkdir(exist_ok=True)
                
                new_path = batch_folder / file_path.name
                new_path = self.get_unique_filename(new_path)
                shutil.move(str(file_path), str(new_path))
                
                result['destination'] = str(new_path.relative_to(destination))
                self.stats['duplicates'] += 1
                return result
                
            # ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÚ¯Ø§Ù‡
            device = self.detect_device(file_path)
            result['device'] = device
            
            if device:
                self.stats['by_device'][device] += 1
                
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ®
            file_date = self.get_file_date(file_path)
            result['date'] = file_date.strftime("%Y-%m-%d %H:%M:%S") if file_date else None
            
            if not file_date:
                # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ unsorted
                unsorted_folder = destination / "Unsorted"
                unsorted_folder.mkdir(exist_ok=True)
                
                # Ø§ÛŒØ¬Ø§Ø¯ batch folder
                batch_num = (len(list(unsorted_folder.glob("*"))) // 500) + 1
                batch_folder = unsorted_folder / f"batch_{batch_num:03d}"
                batch_folder.mkdir(exist_ok=True)
                
                new_path = batch_folder / file_path.name
                new_path = self.get_unique_filename(new_path)
                shutil.move(str(file_path), str(new_path))
                
                result['status'] = 'unsorted'
                result['destination'] = str(new_path.relative_to(destination))
                return result
                
            # Ø§ÛŒØ¬Ø§Ø¯ Ø³Ø§Ø®ØªØ§Ø± ÙÙˆÙ„Ø¯Ø±
            target_folder = self.create_folder_structure(destination, file_date, device)
            
            # ØªØ¹ÛŒÛŒÙ† Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
            if device and self.organization.get('file_naming'):
                pattern = self.organization['file_naming'].get('pattern', '{original}')
                new_name = pattern.format(
                    device=self.devices[device].get('folder_prefix', device),
                    date=file_date.strftime("%Y%m%d"),
                    time=file_date.strftime("%H%M%S"),
                    original=file_path.stem
                ) + file_path.suffix
                new_path = target_folder / new_name
            else:
                new_path = target_folder / file_path.name
                
            # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ù… ØªÚ©Ø±Ø§Ø±ÛŒ
            new_path = self.get_unique_filename(new_path)
            
            # Ø§Ù†ØªÙ‚Ø§Ù„ ÙØ§ÛŒÙ„
            shutil.move(str(file_path), str(new_path))
            
            # Ø«Ø¨Øª Ù‡Ø´
            existing_hashes[file_hash] = new_path
            
            result['status'] = 'organized'
            result['destination'] = str(new_path.relative_to(destination))
            
            # Ø¢Ù…Ø§Ø±
            self.stats['organized'] += 1
            date_key = file_date.strftime("%Y-%m-%d")
            self.stats['by_date'][date_key] += 1
            
            # Ù†ÙˆØ¹ ÙØ§ÛŒÙ„
            if self.is_image(file_path):
                self.stats['by_type']['images'] += 1
            elif self.is_video(file_path):
                self.stats['by_type']['videos'] += 1
            elif self.is_raw(file_path):
                self.stats['by_type']['raw'] += 1
                
            return result
            
        except Exception as e:
            result['status'] = 'error'
            result['error'] = str(e)
            self.stats['errors'] += 1
            self.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path.name}: {e}")
            return result
            
    def get_unique_filename(self, file_path: Path) -> Path:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø§Ù… Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯"""
        if not file_path.exists():
            return file_path
            
        counter = 1
        stem = file_path.stem
        suffix = file_path.suffix
        parent = file_path.parent
        
        while file_path.exists():
            new_name = f"{stem}_{counter:03d}{suffix}"
            file_path = parent / new_name
            counter += 1
            
        return file_path
        
    def organize_files(self, source_folder: str, destination_folder: str, 
                       recursive: bool = True):
        """Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§"""
        source_path = Path(source_folder)
        dest_path = Path(destination_folder)
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙÙˆÙ„Ø¯Ø± Ù…Ù†Ø¨Ø¹
        if not source_path.exists():
            self.logger.error(f"ÙÙˆÙ„Ø¯Ø± Ù…Ù†Ø¨Ø¹ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {source_path}")
            return
            
        # Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø± Ù…Ù‚ØµØ¯
        dest_path.mkdir(parents=True, exist_ok=True)
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        if recursive:
            files = list(source_path.rglob("*"))
        else:
            files = list(source_path.glob("*"))
            
        files = [f for f in files if f.is_file()]
        
        self.stats['total_files'] = len(files)
        self.logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {len(files)}")
        
        if not files:
            self.logger.warning("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return
            
        # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù‡Ø´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØªÚ©Ø±Ø§Ø±ÛŒ
        existing_hashes = {}
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ
        if self.processing.get('parallel_processing', True):
            max_workers = self.processing.get('max_workers', 4)
            self.logger.info(f"Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ø¨Ø§ {max_workers} ÙˆØ±Ú©Ø±")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = []
                
                for file_path in files:
                    future = executor.submit(
                        self.organize_single_file,
                        file_path,
                        dest_path,
                        existing_hashes
                    )
                    futures.append(future)
                    
                # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬
                for i, future in enumerate(as_completed(futures), 1):
                    result = future.result()
                    self.stats['processed'] += 1
                    
                    # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
                    if i % 50 == 0 or i == len(files):
                        progress = (i / len(files)) * 100
                        self.logger.info(
                            f"Ù¾ÛŒØ´Ø±ÙØª: {progress:.1f}% ({i}/{len(files)}) | "
                            f"Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ: {self.stats['organized']} | "
                            f"ØªÚ©Ø±Ø§Ø±ÛŒ: {self.stats['duplicates']} | "
                            f"Ø®Ø·Ø§: {self.stats['errors']}"
                        )
        else:
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ§Ù„
            for i, file_path in enumerate(files, 1):
                result = self.organize_single_file(file_path, dest_path, existing_hashes)
                self.stats['processed'] += 1
                
                # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
                if i % 50 == 0 or i == len(files):
                    progress = (i / len(files)) * 100
                    self.logger.info(
                        f"Ù¾ÛŒØ´Ø±ÙØª: {progress:.1f}% ({i}/{len(files)}) | "
                        f"Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ: {self.stats['organized']} | "
                        f"ØªÚ©Ø±Ø§Ø±ÛŒ: {self.stats['duplicates']} | "
                        f"Ø®Ø·Ø§: {self.stats['errors']}"
                    )
                    
    def generate_report(self, output_file: str = "organization_report.json"):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'statistics': self.stats,
            'devices_detected': list(self.stats['by_device'].keys()),
            'dates_processed': list(self.stats['by_date'].keys()),
            'summary': {
                'success_rate': (self.stats['organized'] / self.stats['total_files'] * 100) 
                                if self.stats['total_files'] > 0 else 0,
                'duplicate_rate': (self.stats['duplicates'] / self.stats['total_files'] * 100)
                                  if self.stats['total_files'] > 0 else 0,
                'error_rate': (self.stats['errors'] / self.stats['total_files'] * 100)
                              if self.stats['total_files'] > 0 else 0
            }
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
        self.logger.info(f"Ú¯Ø²Ø§Ø±Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_file}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ø¹Ù…Ù„ÛŒØ§Øª:")
        self.logger.info(f"  â€¢ Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {self.stats['total_files']}")
        self.logger.info(f"  â€¢ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {self.stats['processed']}")
        self.logger.info(f"  â€¢ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø´Ø¯Ù‡: {self.stats['organized']}")
        self.logger.info(f"  â€¢ ØªÚ©Ø±Ø§Ø±ÛŒ: {self.stats['duplicates']}")
        self.logger.info(f"  â€¢ Ø®Ø·Ø§: {self.stats['errors']}")
        self.logger.info("")
        self.logger.info("ğŸ“± Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡:")
        for device, count in self.stats['by_device'].items():
            device_name = self.devices[device].get('name', device)
            self.logger.info(f"  â€¢ {device_name}: {count} ÙØ§ÛŒÙ„")
        self.logger.info("")
        self.logger.info("ğŸ“… ØªÙˆØ²ÛŒØ¹ ØªØ§Ø±ÛŒØ®ÛŒ:")
        sorted_dates = sorted(self.stats['by_date'].items())
        for date, count in sorted_dates[-10:]:  # Ù†Ù…Ø§ÛŒØ´ 10 Ø±ÙˆØ² Ø¢Ø®Ø±
            self.logger.info(f"  â€¢ {date}: {count} ÙØ§ÛŒÙ„")
        self.logger.info("")
        self.logger.info("ğŸ“ Ø§Ù†ÙˆØ§Ø¹ ÙØ§ÛŒÙ„:")
        for file_type, count in self.stats['by_type'].items():
            self.logger.info(f"  â€¢ {file_type}: {count} ÙØ§ÛŒÙ„")
        self.logger.info("=" * 80)
        
        return report


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    print("=" * 80)
    print("ğŸ¯ Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø¹Ú©Ø³ Ùˆ ÙÛŒÙ„Ù…")
    print("ğŸ“± Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² 5 Ø¯ÙˆØ±Ø¨ÛŒÙ† Ùˆ Ù…ÙˆØ¨Ø§ÛŒÙ„ Ù…Ø®ØªÙ„Ù")
    print("=" * 80)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†
    if len(sys.argv) > 1:
        source = sys.argv[1]
        destination = sys.argv[2] if len(sys.argv) > 2 else "organized_photos"
        config_file = sys.argv[3] if len(sys.argv) > 3 else "device_config.yaml"
    else:
        # Ø¯Ø±ÛŒØ§ÙØª ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ú©Ø§Ø±Ø¨Ø±
        print("\nğŸ“ ØªÙ†Ø¸ÛŒÙ…Ø§Øª:")
        source = input("Ù…Ø³ÛŒØ± ÙÙˆÙ„Ø¯Ø± Ù…Ù†Ø¨Ø¹: ").strip()
        destination = input("Ù…Ø³ÛŒØ± ÙÙˆÙ„Ø¯Ø± Ù…Ù‚ØµØ¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: organized_photos): ").strip() or "organized_photos"
        config_file = input("ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: device_config.yaml): ").strip() or "device_config.yaml"
        
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙÙˆÙ„Ø¯Ø± Ù…Ù†Ø¨Ø¹
    source_path = Path(source)
    if not source_path.exists():
        print(f"âŒ ÙÙˆÙ„Ø¯Ø± Ù…Ù†Ø¨Ø¹ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {source}")
        return
        
    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
    print(f"\nâœ… ÙÙˆÙ„Ø¯Ø± Ù…Ù†Ø¨Ø¹: {source}")
    print(f"âœ… ÙÙˆÙ„Ø¯Ø± Ù…Ù‚ØµØ¯: {destination}")
    print(f"âœ… ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ: {config_file}")
    
    # ØªØ£ÛŒÛŒØ¯
    confirm = input("\nØ¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ù‡ÛŒØ¯ØŸ (y/n): ").strip().lower()
    if confirm not in ['y', 'yes', 'Ø¨Ù„Ù‡']:
        print("âŒ Ø¹Ù…Ù„ÛŒØ§Øª Ù„ØºÙˆ Ø´Ø¯!")
        return
        
    # Ø´Ø±ÙˆØ¹ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ
    print("\nğŸš€ Ø´Ø±ÙˆØ¹ Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ...")
    start_time = time.time()
    
    try:
        organizer = EnhancedPhotoOrganizer(config_file)
        organizer.organize_files(source, destination)
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
        report_file = Path(destination) / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        organizer.generate_report(str(report_file))
        
        elapsed_time = time.time() - start_time
        print(f"\nâœ… Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
        print(f"â±ï¸ Ø²Ù…Ø§Ù† Ú©Ù„: {elapsed_time/60:.1f} Ø¯Ù‚ÛŒÙ‚Ù‡")
        print(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {report_file}")
        
    except KeyboardInterrupt:
        print("\n\nâŒ Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯!")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()